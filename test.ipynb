{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyzhou/anaconda3/envs/convnext/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from models.convnext import convnext_base,convnext_large\n",
    "import torch\n",
    "model=convnext_large(num_classes=6).cuda()\n",
    "ckp=torch.load('/home/xyzhou/code/docker_demo/scripts/best.pth')\n",
    "model.load_state_dict(ckp['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.convnext import convnext_base,convnext_large\n",
    "import torch\n",
    "model=convnext_base(num_classes=5).cuda()\n",
    "ckp=torch.load('/home/xyzhou/code/aisc/traincode/convnext/result/result2/checkpoint-29.pth')\n",
    "model.load_state_dict(ckp['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (downsample_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (stages): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (12): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (13): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (14): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (15): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (16): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (17): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (18): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (19): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (20): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (21): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (22): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (23): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (24): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (25): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (26): Block(\n",
       "        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU()\n",
       "        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelc=convnext_base(num_classes=4).cuda()\n",
    "ckp=torch.load('/home/xyzhou/code/aisc/traincode/convnext/result/result_n4/checkpoint-59.pth')\n",
    "modelc.load_state_dict(ckp['model'])\n",
    "modelc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose,ToTensor,Normalize,Resize\n",
    "transform=Compose([Resize((224,224)),ToTensor(),Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "dataset=ImageFolder('/old_home/xyzhou/aisc/images/train/u2l',transform=transform)\n",
    "dp=ImageFolder('/old_home/xyzhou/aisc/images/train/u2l')\n",
    "data_loader = DataLoader(dataset=dataset,batch_size=64,num_workers=4,shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "ota,otac=[],[]\n",
    "with torch.no_grad():\n",
    "    for i in data_loader:\n",
    "        outc=modelc.forward_features(i[0].cuda())\n",
    "        oc=np.array(outc.detach().cpu())\n",
    "        out=model(i[0].cuda())\n",
    "        o=np.array(torch.max(out,dim=1).indices.detach().cpu())\n",
    "        ota.extend(o)\n",
    "        otac.extend(oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 750, 2: 397, 1: 349, 6: 144, 5: 418, 0: 133, 7: 403, 3: 105})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X=np.array(otac)\n",
    "k = KMeans(n_clusters=8, random_state=1).fit(X)\n",
    "from collections import Counter\n",
    "a=k.predict(X)\n",
    "Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 1441, 0: 324, 3: 353, 2: 279, 1: 302})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "aota=np.array(ota)\n",
    "Counter(aota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(a==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(aota==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(133, 324, 133), (349, 324, 349), (397, 324, 73), (105, 324, 105), (750, 324, 750), (418, 324, 418), (144, 324, 144), (403, 324, 403)]\n",
      "[(133, 302, 132), (349, 302, 349), (397, 302, 397), (105, 302, 105), (750, 302, 750), (418, 302, 117), (144, 302, 144), (403, 302, 403)]\n",
      "[(133, 279, 133), (349, 279, 334), (397, 279, 397), (105, 279, 105), (750, 279, 486), (418, 279, 418), (144, 279, 144), (403, 279, 403)]\n",
      "[(133, 353, 133), (349, 353, 349), (397, 353, 397), (105, 353, 103), (750, 353, 749), (418, 353, 418), (144, 353, 144), (403, 353, 53)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    n=[]\n",
    "    for j in range(8):\n",
    "        f2=np.where(aota==i)\n",
    "        f1=np.where(a==j)\n",
    "        # if len(set(f1[0])-set(f2[0]))>(len(f1[0])-10) and\\\n",
    "        #     len(set(f1[0])-set(f2[0]))<(len(f1[0])+10):continue\n",
    "        # else:    \n",
    "        #     n.append(j)\n",
    "        # if len(set(f1[0])-set(f2[0]))!=len(f1[0]):n.append(j)\n",
    "        # n.append(len(set(f1[0])-set(f2[0])))\n",
    "        n.append((len(set(f1[0])),len(set(f2[0])),len(set(f1[0])-set(f2[0]))))\n",
    "    print(n)\n",
    "    # print(f'aota{i} = a{int(np.where(np.array(n)==min(n))[0])} difnum:{min(n)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(aota==3)##对应聚类a3   a:aota:a-aota=415 353 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(aota==2)##对应聚类的a2 a:aota:a-aota=728 279 489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(aota==0)##对应聚类的a4 a:aota:a-aota=394 324 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(aota==1)##对应聚类的a5 a:aota:a-aota=430 302 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X=np.array(ota)\n",
    "k = KMeans(n_clusters=8, random_state=1).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 728, 4: 394, 1: 190, 5: 430, 0: 226, 7: 184, 3: 415, 6: 132})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a=k.predict(X)\n",
    "Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   13,   38,   50,   65,   67,   71,   72,   82,   84,   86,\n",
       "         90,   96,   97,   99,  110,  121,  122,  124,  136,  137,  142,\n",
       "        143,  148,  152,  158,  171,  175,  179,  180,  182,  199,  200,\n",
       "        201,  202,  203,  206,  207,  208,  209,  210,  212,  224,  226,\n",
       "        227,  230,  233,  237,  239,  243,  251,  260,  267,  268,  269,\n",
       "        273,  274,  287,  297,  299,  306,  309,  324,  327,  331,  347,\n",
       "        352,  360,  361,  362,  364,  370,  384,  391,  392,  406,  408,\n",
       "        410,  414,  415,  420,  422,  426,  428,  430,  478,  486,  489,\n",
       "        491,  506,  515,  516,  522,  523,  527,  528,  531,  541,  544,\n",
       "        554,  573,  629,  632,  634,  638,  641,  645,  653,  675,  676,\n",
       "        682,  685,  688,  720,  729,  744,  772,  773,  779,  785,  788,\n",
       "        791,  808,  810,  835,  841,  844,  846,  853,  855,  862,  864,\n",
       "        868,  870,  873,  894,  896,  898,  902,  905,  910,  915,  919,\n",
       "        925,  926,  945,  952,  954,  955,  969,  978,  991,  998, 1001,\n",
       "       1004, 1006, 1014, 1015, 1017, 1021, 1032, 1039, 1044, 1049, 1055,\n",
       "       1066, 1072, 1076, 1084, 1088, 1093, 1096, 1104, 1107, 1115, 1119,\n",
       "       1121, 1128, 1158, 1183, 1189, 1191, 1207, 1216, 1222, 1231, 1238,\n",
       "       1243, 1253, 1254, 1256, 1262, 1264, 1266, 1267, 1270, 1276, 1285,\n",
       "       1287, 1288, 1293, 1294, 1295, 1297, 1299, 1306, 1308, 1309, 1322,\n",
       "       1326, 1327, 1351, 1360, 1362, 1371, 1372, 1388, 1389, 1395, 1404,\n",
       "       1413, 1440, 1447, 1464, 1481, 1492, 1495, 1497, 1510, 1514, 1518,\n",
       "       1537, 1539, 1547, 1553, 1557, 1560, 1561, 1573, 1574, 1579, 1584,\n",
       "       1585, 1600, 1601, 1605, 1614, 1618, 1625, 1627, 1628, 1630, 1638,\n",
       "       1660, 1664, 1670, 1709, 1714, 1716, 1717, 1719, 1721, 1733, 1736,\n",
       "       1738, 1746, 1752, 1756, 1761, 1775, 1779, 1803, 1807, 1829, 1832,\n",
       "       1834, 1835, 1841, 1842, 1843, 1844, 1849, 1851, 1870, 1881, 1882,\n",
       "       1885, 1903, 1906, 1928, 1934, 1947, 1952, 1955, 1956, 1961, 1962,\n",
       "       1971, 1973, 1975, 1977, 1985, 1999, 2000, 2002, 2003, 2009, 2017,\n",
       "       2025, 2027, 2030, 2042, 2055, 2061, 2065, 2072, 2075, 2085, 2121,\n",
       "       2128, 2133, 2145, 2164, 2176, 2178, 2186, 2201, 2215, 2220, 2223,\n",
       "       2227, 2231, 2239, 2250, 2251, 2253, 2260, 2264, 2271, 2287, 2293,\n",
       "       2310, 2314, 2322, 2324, 2344, 2348, 2360, 2365, 2367, 2381, 2389,\n",
       "       2393, 2418, 2419, 2428, 2442, 2454, 2463, 2468, 2474, 2477, 2479,\n",
       "       2481, 2489, 2498, 2504, 2505, 2516, 2519, 2521, 2531, 2543, 2545,\n",
       "       2547, 2559, 2578, 2586, 2587, 2591, 2592, 2596, 2605, 2610, 2611,\n",
       "       2628, 2631, 2635, 2641, 2647, 2649, 2675, 2683, 2696])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(a==4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xyzhou/code/aisc/traincode/convnext/test.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.29.150.48/home/xyzhou/code/aisc/traincode/convnext/test.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39mwhere(a\u001b[39m==\u001b[39m\u001b[39m2\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "np.where(a==2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlabeled Classify_n9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind:0  num:392\n",
      "kind:1  num:622\n",
      "kind:2  num:1219\n",
      "kind:3  num:451\n",
      "kind:5  num:4\n",
      "kind:6  num:2\n",
      "kind:7  num:6\n",
      "kind:8  num:3\n"
     ]
    }
   ],
   "source": [
    "kind=set(ota)\n",
    "for i in kind:\n",
    "    print(f'kind:{i}  num:{ota.count(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind:0  num:449\n",
      "kind:1  num:800\n",
      "kind:2  num:397\n",
      "kind:3  num:391\n",
      "kind:4  num:198\n",
      "kind:5  num:262\n",
      "kind:6  num:103\n",
      "kind:7  num:99\n"
     ]
    }
   ],
   "source": [
    "# cluster n9\n",
    "kind=set(kl)\n",
    "for i in kind:\n",
    "    print(f'kind:{i}  num:{kl.count(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlabeled Classify_n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind:0  num:396\n",
      "kind:1  num:674\n",
      "kind:2  num:1114\n",
      "kind:3  num:515\n"
     ]
    }
   ],
   "source": [
    "kind=set(ota)\n",
    "for i in kind:\n",
    "    print(f'kind:{i}  num:{ota.count(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind:0  num:226\n",
      "kind:1  num:190\n",
      "kind:2  num:728\n",
      "kind:3  num:415\n",
      "kind:4  num:394\n",
      "kind:5  num:430\n",
      "kind:6  num:132\n",
      "kind:7  num:184\n"
     ]
    }
   ],
   "source": [
    "# cluster n5_n4\n",
    "kind=set(kl)\n",
    "for i in kind:\n",
    "    print(f'kind:{i}  num:{kl.count(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlabeled Classify_n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind:0  num:324\n",
      "kind:1  num:302\n",
      "kind:2  num:279\n",
      "kind:3  num:353\n",
      "kind:4  num:1441\n"
     ]
    }
   ],
   "source": [
    "kind=set(ota)\n",
    "for i in kind:\n",
    "    print(f'kind:{i}  num:{ota.count(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [2, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2          [2, 64, 112, 112]             128\n",
      "              ReLU-3          [2, 64, 112, 112]               0\n",
      "         MaxPool2d-4            [2, 64, 56, 56]               0\n",
      "            Conv2d-5            [2, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6            [2, 64, 56, 56]             128\n",
      "              ReLU-7            [2, 64, 56, 56]               0\n",
      "            Conv2d-8            [2, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9            [2, 64, 56, 56]             128\n",
      "             ReLU-10            [2, 64, 56, 56]               0\n",
      "           Conv2d-11           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12           [2, 256, 56, 56]             512\n",
      "           Conv2d-13           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14           [2, 256, 56, 56]             512\n",
      "             ReLU-15           [2, 256, 56, 56]               0\n",
      "       Bottleneck-16           [2, 256, 56, 56]               0\n",
      "           Conv2d-17            [2, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18            [2, 64, 56, 56]             128\n",
      "             ReLU-19            [2, 64, 56, 56]               0\n",
      "           Conv2d-20            [2, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21            [2, 64, 56, 56]             128\n",
      "             ReLU-22            [2, 64, 56, 56]               0\n",
      "           Conv2d-23           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24           [2, 256, 56, 56]             512\n",
      "             ReLU-25           [2, 256, 56, 56]               0\n",
      "       Bottleneck-26           [2, 256, 56, 56]               0\n",
      "           Conv2d-27            [2, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28            [2, 64, 56, 56]             128\n",
      "             ReLU-29            [2, 64, 56, 56]               0\n",
      "           Conv2d-30            [2, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31            [2, 64, 56, 56]             128\n",
      "             ReLU-32            [2, 64, 56, 56]               0\n",
      "           Conv2d-33           [2, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34           [2, 256, 56, 56]             512\n",
      "             ReLU-35           [2, 256, 56, 56]               0\n",
      "       Bottleneck-36           [2, 256, 56, 56]               0\n",
      "           Conv2d-37           [2, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38           [2, 128, 56, 56]             256\n",
      "             ReLU-39           [2, 128, 56, 56]               0\n",
      "           Conv2d-40           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41           [2, 128, 28, 28]             256\n",
      "             ReLU-42           [2, 128, 28, 28]               0\n",
      "           Conv2d-43           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44           [2, 512, 28, 28]           1,024\n",
      "           Conv2d-45           [2, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46           [2, 512, 28, 28]           1,024\n",
      "             ReLU-47           [2, 512, 28, 28]               0\n",
      "       Bottleneck-48           [2, 512, 28, 28]               0\n",
      "           Conv2d-49           [2, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50           [2, 128, 28, 28]             256\n",
      "             ReLU-51           [2, 128, 28, 28]               0\n",
      "           Conv2d-52           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53           [2, 128, 28, 28]             256\n",
      "             ReLU-54           [2, 128, 28, 28]               0\n",
      "           Conv2d-55           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56           [2, 512, 28, 28]           1,024\n",
      "             ReLU-57           [2, 512, 28, 28]               0\n",
      "       Bottleneck-58           [2, 512, 28, 28]               0\n",
      "           Conv2d-59           [2, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60           [2, 128, 28, 28]             256\n",
      "             ReLU-61           [2, 128, 28, 28]               0\n",
      "           Conv2d-62           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63           [2, 128, 28, 28]             256\n",
      "             ReLU-64           [2, 128, 28, 28]               0\n",
      "           Conv2d-65           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66           [2, 512, 28, 28]           1,024\n",
      "             ReLU-67           [2, 512, 28, 28]               0\n",
      "       Bottleneck-68           [2, 512, 28, 28]               0\n",
      "           Conv2d-69           [2, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70           [2, 128, 28, 28]             256\n",
      "             ReLU-71           [2, 128, 28, 28]               0\n",
      "           Conv2d-72           [2, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73           [2, 128, 28, 28]             256\n",
      "             ReLU-74           [2, 128, 28, 28]               0\n",
      "           Conv2d-75           [2, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76           [2, 512, 28, 28]           1,024\n",
      "             ReLU-77           [2, 512, 28, 28]               0\n",
      "       Bottleneck-78           [2, 512, 28, 28]               0\n",
      "           Conv2d-79           [2, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80           [2, 256, 28, 28]             512\n",
      "             ReLU-81           [2, 256, 28, 28]               0\n",
      "           Conv2d-82           [2, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83           [2, 256, 14, 14]             512\n",
      "             ReLU-84           [2, 256, 14, 14]               0\n",
      "           Conv2d-85          [2, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86          [2, 1024, 14, 14]           2,048\n",
      "           Conv2d-87          [2, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88          [2, 1024, 14, 14]           2,048\n",
      "             ReLU-89          [2, 1024, 14, 14]               0\n",
      "       Bottleneck-90          [2, 1024, 14, 14]               0\n",
      "           Conv2d-91           [2, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92           [2, 256, 14, 14]             512\n",
      "             ReLU-93           [2, 256, 14, 14]               0\n",
      "           Conv2d-94           [2, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95           [2, 256, 14, 14]             512\n",
      "             ReLU-96           [2, 256, 14, 14]               0\n",
      "           Conv2d-97          [2, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98          [2, 1024, 14, 14]           2,048\n",
      "             ReLU-99          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-100          [2, 1024, 14, 14]               0\n",
      "          Conv2d-101           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102           [2, 256, 14, 14]             512\n",
      "            ReLU-103           [2, 256, 14, 14]               0\n",
      "          Conv2d-104           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105           [2, 256, 14, 14]             512\n",
      "            ReLU-106           [2, 256, 14, 14]               0\n",
      "          Conv2d-107          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-109          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-110          [2, 1024, 14, 14]               0\n",
      "          Conv2d-111           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112           [2, 256, 14, 14]             512\n",
      "            ReLU-113           [2, 256, 14, 14]               0\n",
      "          Conv2d-114           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115           [2, 256, 14, 14]             512\n",
      "            ReLU-116           [2, 256, 14, 14]               0\n",
      "          Conv2d-117          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-119          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-120          [2, 1024, 14, 14]               0\n",
      "          Conv2d-121           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122           [2, 256, 14, 14]             512\n",
      "            ReLU-123           [2, 256, 14, 14]               0\n",
      "          Conv2d-124           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125           [2, 256, 14, 14]             512\n",
      "            ReLU-126           [2, 256, 14, 14]               0\n",
      "          Conv2d-127          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-129          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-130          [2, 1024, 14, 14]               0\n",
      "          Conv2d-131           [2, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132           [2, 256, 14, 14]             512\n",
      "            ReLU-133           [2, 256, 14, 14]               0\n",
      "          Conv2d-134           [2, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135           [2, 256, 14, 14]             512\n",
      "            ReLU-136           [2, 256, 14, 14]               0\n",
      "          Conv2d-137          [2, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138          [2, 1024, 14, 14]           2,048\n",
      "            ReLU-139          [2, 1024, 14, 14]               0\n",
      "      Bottleneck-140          [2, 1024, 14, 14]               0\n",
      "          Conv2d-141           [2, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142           [2, 512, 14, 14]           1,024\n",
      "            ReLU-143           [2, 512, 14, 14]               0\n",
      "          Conv2d-144             [2, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145             [2, 512, 7, 7]           1,024\n",
      "            ReLU-146             [2, 512, 7, 7]               0\n",
      "          Conv2d-147            [2, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148            [2, 2048, 7, 7]           4,096\n",
      "          Conv2d-149            [2, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150            [2, 2048, 7, 7]           4,096\n",
      "            ReLU-151            [2, 2048, 7, 7]               0\n",
      "      Bottleneck-152            [2, 2048, 7, 7]               0\n",
      "          Conv2d-153             [2, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154             [2, 512, 7, 7]           1,024\n",
      "            ReLU-155             [2, 512, 7, 7]               0\n",
      "          Conv2d-156             [2, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157             [2, 512, 7, 7]           1,024\n",
      "            ReLU-158             [2, 512, 7, 7]               0\n",
      "          Conv2d-159            [2, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160            [2, 2048, 7, 7]           4,096\n",
      "            ReLU-161            [2, 2048, 7, 7]               0\n",
      "      Bottleneck-162            [2, 2048, 7, 7]               0\n",
      "          Conv2d-163             [2, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164             [2, 512, 7, 7]           1,024\n",
      "            ReLU-165             [2, 512, 7, 7]               0\n",
      "          Conv2d-166             [2, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167             [2, 512, 7, 7]           1,024\n",
      "            ReLU-168             [2, 512, 7, 7]               0\n",
      "          Conv2d-169            [2, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170            [2, 2048, 7, 7]           4,096\n",
      "            ReLU-171            [2, 2048, 7, 7]               0\n",
      "      Bottleneck-172            [2, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173            [2, 2048, 1, 1]               0\n",
      "          Linear-174                  [2, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.15\n",
      "Forward/backward pass size (MB): 573.12\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 671.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "model=models.resnet50()\n",
    "summary(model, input_size=[(3,224,224)], batch_size=2, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [2, 128, 56, 56]           6,272\n",
      "         LayerNorm-2           [2, 128, 56, 56]             256\n",
      "            Conv2d-3           [2, 128, 56, 56]           6,400\n",
      "         LayerNorm-4           [2, 56, 56, 128]             256\n",
      "            Linear-5           [2, 56, 56, 512]          66,048\n",
      "              GELU-6           [2, 56, 56, 512]               0\n",
      "            Linear-7           [2, 56, 56, 128]          65,664\n",
      "          Identity-8           [2, 128, 56, 56]               0\n",
      "             Block-9           [2, 128, 56, 56]               0\n",
      "           Conv2d-10           [2, 128, 56, 56]           6,400\n",
      "        LayerNorm-11           [2, 56, 56, 128]             256\n",
      "           Linear-12           [2, 56, 56, 512]          66,048\n",
      "             GELU-13           [2, 56, 56, 512]               0\n",
      "           Linear-14           [2, 56, 56, 128]          65,664\n",
      "         Identity-15           [2, 128, 56, 56]               0\n",
      "            Block-16           [2, 128, 56, 56]               0\n",
      "           Conv2d-17           [2, 128, 56, 56]           6,400\n",
      "        LayerNorm-18           [2, 56, 56, 128]             256\n",
      "           Linear-19           [2, 56, 56, 512]          66,048\n",
      "             GELU-20           [2, 56, 56, 512]               0\n",
      "           Linear-21           [2, 56, 56, 128]          65,664\n",
      "         Identity-22           [2, 128, 56, 56]               0\n",
      "            Block-23           [2, 128, 56, 56]               0\n",
      "        LayerNorm-24           [2, 128, 56, 56]             256\n",
      "           Conv2d-25           [2, 256, 28, 28]         131,328\n",
      "           Conv2d-26           [2, 256, 28, 28]          12,800\n",
      "        LayerNorm-27           [2, 28, 28, 256]             512\n",
      "           Linear-28          [2, 28, 28, 1024]         263,168\n",
      "             GELU-29          [2, 28, 28, 1024]               0\n",
      "           Linear-30           [2, 28, 28, 256]         262,400\n",
      "         Identity-31           [2, 256, 28, 28]               0\n",
      "            Block-32           [2, 256, 28, 28]               0\n",
      "           Conv2d-33           [2, 256, 28, 28]          12,800\n",
      "        LayerNorm-34           [2, 28, 28, 256]             512\n",
      "           Linear-35          [2, 28, 28, 1024]         263,168\n",
      "             GELU-36          [2, 28, 28, 1024]               0\n",
      "           Linear-37           [2, 28, 28, 256]         262,400\n",
      "         Identity-38           [2, 256, 28, 28]               0\n",
      "            Block-39           [2, 256, 28, 28]               0\n",
      "           Conv2d-40           [2, 256, 28, 28]          12,800\n",
      "        LayerNorm-41           [2, 28, 28, 256]             512\n",
      "           Linear-42          [2, 28, 28, 1024]         263,168\n",
      "             GELU-43          [2, 28, 28, 1024]               0\n",
      "           Linear-44           [2, 28, 28, 256]         262,400\n",
      "         Identity-45           [2, 256, 28, 28]               0\n",
      "            Block-46           [2, 256, 28, 28]               0\n",
      "        LayerNorm-47           [2, 256, 28, 28]             512\n",
      "           Conv2d-48           [2, 512, 14, 14]         524,800\n",
      "           Conv2d-49           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-50           [2, 14, 14, 512]           1,024\n",
      "           Linear-51          [2, 14, 14, 2048]       1,050,624\n",
      "             GELU-52          [2, 14, 14, 2048]               0\n",
      "           Linear-53           [2, 14, 14, 512]       1,049,088\n",
      "         Identity-54           [2, 512, 14, 14]               0\n",
      "            Block-55           [2, 512, 14, 14]               0\n",
      "           Conv2d-56           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-57           [2, 14, 14, 512]           1,024\n",
      "           Linear-58          [2, 14, 14, 2048]       1,050,624\n",
      "             GELU-59          [2, 14, 14, 2048]               0\n",
      "           Linear-60           [2, 14, 14, 512]       1,049,088\n",
      "         Identity-61           [2, 512, 14, 14]               0\n",
      "            Block-62           [2, 512, 14, 14]               0\n",
      "           Conv2d-63           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-64           [2, 14, 14, 512]           1,024\n",
      "           Linear-65          [2, 14, 14, 2048]       1,050,624\n",
      "             GELU-66          [2, 14, 14, 2048]               0\n",
      "           Linear-67           [2, 14, 14, 512]       1,049,088\n",
      "         Identity-68           [2, 512, 14, 14]               0\n",
      "            Block-69           [2, 512, 14, 14]               0\n",
      "           Conv2d-70           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-71           [2, 14, 14, 512]           1,024\n",
      "           Linear-72          [2, 14, 14, 2048]       1,050,624\n",
      "             GELU-73          [2, 14, 14, 2048]               0\n",
      "           Linear-74           [2, 14, 14, 512]       1,049,088\n",
      "         Identity-75           [2, 512, 14, 14]               0\n",
      "            Block-76           [2, 512, 14, 14]               0\n",
      "           Conv2d-77           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-78           [2, 14, 14, 512]           1,024\n",
      "           Linear-79          [2, 14, 14, 2048]       1,050,624\n",
      "             GELU-80          [2, 14, 14, 2048]               0\n",
      "           Linear-81           [2, 14, 14, 512]       1,049,088\n",
      "         Identity-82           [2, 512, 14, 14]               0\n",
      "            Block-83           [2, 512, 14, 14]               0\n",
      "           Conv2d-84           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-85           [2, 14, 14, 512]           1,024\n",
      "           Linear-86          [2, 14, 14, 2048]       1,050,624\n",
      "             GELU-87          [2, 14, 14, 2048]               0\n",
      "           Linear-88           [2, 14, 14, 512]       1,049,088\n",
      "         Identity-89           [2, 512, 14, 14]               0\n",
      "            Block-90           [2, 512, 14, 14]               0\n",
      "           Conv2d-91           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-92           [2, 14, 14, 512]           1,024\n",
      "           Linear-93          [2, 14, 14, 2048]       1,050,624\n",
      "             GELU-94          [2, 14, 14, 2048]               0\n",
      "           Linear-95           [2, 14, 14, 512]       1,049,088\n",
      "         Identity-96           [2, 512, 14, 14]               0\n",
      "            Block-97           [2, 512, 14, 14]               0\n",
      "           Conv2d-98           [2, 512, 14, 14]          25,600\n",
      "        LayerNorm-99           [2, 14, 14, 512]           1,024\n",
      "          Linear-100          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-101          [2, 14, 14, 2048]               0\n",
      "          Linear-102           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-103           [2, 512, 14, 14]               0\n",
      "           Block-104           [2, 512, 14, 14]               0\n",
      "          Conv2d-105           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-106           [2, 14, 14, 512]           1,024\n",
      "          Linear-107          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-108          [2, 14, 14, 2048]               0\n",
      "          Linear-109           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-110           [2, 512, 14, 14]               0\n",
      "           Block-111           [2, 512, 14, 14]               0\n",
      "          Conv2d-112           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-113           [2, 14, 14, 512]           1,024\n",
      "          Linear-114          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-115          [2, 14, 14, 2048]               0\n",
      "          Linear-116           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-117           [2, 512, 14, 14]               0\n",
      "           Block-118           [2, 512, 14, 14]               0\n",
      "          Conv2d-119           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-120           [2, 14, 14, 512]           1,024\n",
      "          Linear-121          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-122          [2, 14, 14, 2048]               0\n",
      "          Linear-123           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-124           [2, 512, 14, 14]               0\n",
      "           Block-125           [2, 512, 14, 14]               0\n",
      "          Conv2d-126           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-127           [2, 14, 14, 512]           1,024\n",
      "          Linear-128          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-129          [2, 14, 14, 2048]               0\n",
      "          Linear-130           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-131           [2, 512, 14, 14]               0\n",
      "           Block-132           [2, 512, 14, 14]               0\n",
      "          Conv2d-133           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-134           [2, 14, 14, 512]           1,024\n",
      "          Linear-135          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-136          [2, 14, 14, 2048]               0\n",
      "          Linear-137           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-138           [2, 512, 14, 14]               0\n",
      "           Block-139           [2, 512, 14, 14]               0\n",
      "          Conv2d-140           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-141           [2, 14, 14, 512]           1,024\n",
      "          Linear-142          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-143          [2, 14, 14, 2048]               0\n",
      "          Linear-144           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-145           [2, 512, 14, 14]               0\n",
      "           Block-146           [2, 512, 14, 14]               0\n",
      "          Conv2d-147           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-148           [2, 14, 14, 512]           1,024\n",
      "          Linear-149          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-150          [2, 14, 14, 2048]               0\n",
      "          Linear-151           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-152           [2, 512, 14, 14]               0\n",
      "           Block-153           [2, 512, 14, 14]               0\n",
      "          Conv2d-154           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-155           [2, 14, 14, 512]           1,024\n",
      "          Linear-156          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-157          [2, 14, 14, 2048]               0\n",
      "          Linear-158           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-159           [2, 512, 14, 14]               0\n",
      "           Block-160           [2, 512, 14, 14]               0\n",
      "          Conv2d-161           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-162           [2, 14, 14, 512]           1,024\n",
      "          Linear-163          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-164          [2, 14, 14, 2048]               0\n",
      "          Linear-165           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-166           [2, 512, 14, 14]               0\n",
      "           Block-167           [2, 512, 14, 14]               0\n",
      "          Conv2d-168           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-169           [2, 14, 14, 512]           1,024\n",
      "          Linear-170          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-171          [2, 14, 14, 2048]               0\n",
      "          Linear-172           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-173           [2, 512, 14, 14]               0\n",
      "           Block-174           [2, 512, 14, 14]               0\n",
      "          Conv2d-175           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-176           [2, 14, 14, 512]           1,024\n",
      "          Linear-177          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-178          [2, 14, 14, 2048]               0\n",
      "          Linear-179           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-180           [2, 512, 14, 14]               0\n",
      "           Block-181           [2, 512, 14, 14]               0\n",
      "          Conv2d-182           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-183           [2, 14, 14, 512]           1,024\n",
      "          Linear-184          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-185          [2, 14, 14, 2048]               0\n",
      "          Linear-186           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-187           [2, 512, 14, 14]               0\n",
      "           Block-188           [2, 512, 14, 14]               0\n",
      "          Conv2d-189           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-190           [2, 14, 14, 512]           1,024\n",
      "          Linear-191          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-192          [2, 14, 14, 2048]               0\n",
      "          Linear-193           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-194           [2, 512, 14, 14]               0\n",
      "           Block-195           [2, 512, 14, 14]               0\n",
      "          Conv2d-196           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-197           [2, 14, 14, 512]           1,024\n",
      "          Linear-198          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-199          [2, 14, 14, 2048]               0\n",
      "          Linear-200           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-201           [2, 512, 14, 14]               0\n",
      "           Block-202           [2, 512, 14, 14]               0\n",
      "          Conv2d-203           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-204           [2, 14, 14, 512]           1,024\n",
      "          Linear-205          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-206          [2, 14, 14, 2048]               0\n",
      "          Linear-207           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-208           [2, 512, 14, 14]               0\n",
      "           Block-209           [2, 512, 14, 14]               0\n",
      "          Conv2d-210           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-211           [2, 14, 14, 512]           1,024\n",
      "          Linear-212          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-213          [2, 14, 14, 2048]               0\n",
      "          Linear-214           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-215           [2, 512, 14, 14]               0\n",
      "           Block-216           [2, 512, 14, 14]               0\n",
      "          Conv2d-217           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-218           [2, 14, 14, 512]           1,024\n",
      "          Linear-219          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-220          [2, 14, 14, 2048]               0\n",
      "          Linear-221           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-222           [2, 512, 14, 14]               0\n",
      "           Block-223           [2, 512, 14, 14]               0\n",
      "          Conv2d-224           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-225           [2, 14, 14, 512]           1,024\n",
      "          Linear-226          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-227          [2, 14, 14, 2048]               0\n",
      "          Linear-228           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-229           [2, 512, 14, 14]               0\n",
      "           Block-230           [2, 512, 14, 14]               0\n",
      "          Conv2d-231           [2, 512, 14, 14]          25,600\n",
      "       LayerNorm-232           [2, 14, 14, 512]           1,024\n",
      "          Linear-233          [2, 14, 14, 2048]       1,050,624\n",
      "            GELU-234          [2, 14, 14, 2048]               0\n",
      "          Linear-235           [2, 14, 14, 512]       1,049,088\n",
      "        Identity-236           [2, 512, 14, 14]               0\n",
      "           Block-237           [2, 512, 14, 14]               0\n",
      "       LayerNorm-238           [2, 512, 14, 14]           1,024\n",
      "          Conv2d-239            [2, 1024, 7, 7]       2,098,176\n",
      "          Conv2d-240            [2, 1024, 7, 7]          51,200\n",
      "       LayerNorm-241            [2, 7, 7, 1024]           2,048\n",
      "          Linear-242            [2, 7, 7, 4096]       4,198,400\n",
      "            GELU-243            [2, 7, 7, 4096]               0\n",
      "          Linear-244            [2, 7, 7, 1024]       4,195,328\n",
      "        Identity-245            [2, 1024, 7, 7]               0\n",
      "           Block-246            [2, 1024, 7, 7]               0\n",
      "          Conv2d-247            [2, 1024, 7, 7]          51,200\n",
      "       LayerNorm-248            [2, 7, 7, 1024]           2,048\n",
      "          Linear-249            [2, 7, 7, 4096]       4,198,400\n",
      "            GELU-250            [2, 7, 7, 4096]               0\n",
      "          Linear-251            [2, 7, 7, 1024]       4,195,328\n",
      "        Identity-252            [2, 1024, 7, 7]               0\n",
      "           Block-253            [2, 1024, 7, 7]               0\n",
      "          Conv2d-254            [2, 1024, 7, 7]          51,200\n",
      "       LayerNorm-255            [2, 7, 7, 1024]           2,048\n",
      "          Linear-256            [2, 7, 7, 4096]       4,198,400\n",
      "            GELU-257            [2, 7, 7, 4096]               0\n",
      "          Linear-258            [2, 7, 7, 1024]       4,195,328\n",
      "        Identity-259            [2, 1024, 7, 7]               0\n",
      "           Block-260            [2, 1024, 7, 7]               0\n",
      "       LayerNorm-261                  [2, 1024]           2,048\n",
      "          Linear-262                     [2, 5]           5,125\n",
      "================================================================\n",
      "Total params: 87,553,541\n",
      "Trainable params: 87,553,541\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.15\n",
      "Forward/backward pass size (MB): 953.98\n",
      "Params size (MB): 333.99\n",
      "Estimated Total Size (MB): 1289.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=[(3,224,224)], batch_size=2, device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('convnext')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c49f90d8a4f1b6c4ea0a6d0c0215d654b989491a9d43df50e150ce9092e55b54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
